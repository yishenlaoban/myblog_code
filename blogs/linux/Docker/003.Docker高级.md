---
title: Docker高级
date: 2022-06-11
categories:
 - Linux
tags:
 - Docker
sidebar: 'auto'
---
## Docker上软件复杂安装

### 1、Mysql主从复制

:::tip 1.新建主服务器容器3307

```shell
docker run -p 3307:3306 --name mysql-master 
-v /mydata/mysql-master/log:/var/log/mysql 
-v /mydata/mysql-master/data:/var/lib/mysql 
-v /mydata/mysql-master/conf:/etc/mysql 
-e MYSQL_ROOT_PASSWORD=root  
-d mysql:5.7
```

​	![image-20220616113208132](https://img.yishenlaoban.top/image_my/2346254-20220616113208441-1472280881.png)  

:::



:::tip 2.进入/mydata/mysql-master/conf目录下新建my.cnf

```
cd /mydata/mysql-master/conf
vim my.cnf
```

> [mysqld]
>
> \## 设置server_id，同一局域网中需要唯一
>
> server_id=101 
>
> \## 指定不需要同步的数据库名称
>
> binlog-ignore-db=mysql 
>
> \## 开启二进制日志功能
>
> log-bin=mall-mysql-bin 
>
> \## 设置二进制日志使用内存大小（事务）
>
> binlog_cache_size=1M 
>
> \## 设置使用的二进制日志格式（mixed,statement,row）
>
> binlog_format=mixed 
>
> \## 二进制日志过期清理时间。默认值为0，表示不自动清理。
>
> expire_logs_days=7 
>
> \## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。
>
> \## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致
>
> slave_skip_errors=1062

:::



:::tip 3.重启master实例

```shell
docker restart mysql-master
```

![image-20220616150514508](https://img.yishenlaoban.top/image_my/2346254-20220616150514877-1490082655.png) 

:::



:::tip 4.进入mysql-master容器

```shell
docker exec -it mysql-master bash
mysql -uroot -p
```

![image-20220616151024787](https://img.yishenlaoban.top/image_my/2346254-20220616151025019-519565671.png) 

:::



:::tip 5.master容器实例上创建数据同步用户

```shell
create user 'slave'@'%' identified by '123456';
grant replication slave,replication client on *.*to'slave'@'%';
```

![image-20220616151717368](https://img.yishenlaoban.top/image_my/2346254-20220616151717639-1367493840.png) 

:::



:::tip 6.新建从服务器容器3308

```shell
docker run -p 3308:3306 --name mysql-slave 
-v /mydata/mysql-slave/log:/var/log/mysql 
-v /mydata/mysql-slave/data:/var/lib/mysql 
-v /mydata/mysql-slave/conf:/etc/mysql 
-e MYSQL_ROOT_PASSWORD=root  
-d mysql:5.7
```

![image-20220616152706223](https://img.yishenlaoban.top/image_my/2346254-20220616152706619-1042942807.png) 

:::



:::tip 7.进入/mydata/msql-slave/conf 目录下创建 my.cnf 

> [mysqld]
>
> \## 设置server_id，同一局域网中需要唯一
>
> server_id=102
>
> \## 指定不需要同步的数据库名称
>
> binlog-ignore-db=mysql 
>
> \## 开启二进制日志功能，以备Slave作为其它数据库实例的Master时使用
>
> log-bin=mall-mysql-slave1-bin 
>
> \## 设置二进制日志使用内存大小（事务）
>
> binlog_cache_size=1M 
>
> \## 设置使用的二进制日志格式（mixed,statement,row）
>
> binlog_format=mixed 
>
> \## 二进制日志过期清理时间。默认值为0，表示不自动清理。
>
> expire_logs_days=7 
>
> \## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。
>
> \## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致
>
> slave_skip_errors=1062 
>
> \## relay_log配置中继日志
>
> relay_log=mall-mysql-relay-bin 
>
> \## log_slave_updates表示slave将复制事件写进自己的二进制日志
>
> log_slave_updates=1 
>
> \## slave设置为只读（具有super权限的用户除外）
>
> read_only=1

<Badge text='重启slave实例' type='danger'/>

:::



:::tip 8.在主数据库中查看主从同步状态

```mysql
show master status
```

![image-20220616154944717](https://img.yishenlaoban.top/image_my/2346254-20220616154945374-71902092.png) 

:::



:::tip 9.在从数据库设置主从复制

```shell
docker exec -it mysql-slave bash
mysql -uroot -p

#修改配置（认老大）
change master to master_host='宿主机ip', master_user='slave', master_password='123456', master_port=3307, master_log_file='mall-mysql-bin.000001', master_log_pos=617, master_connect_retry=30;
```

![image-20220616160815130](https://img.yishenlaoban.top/image_my/2346254-20220616160815558-541753090.png)

> **`主从复制参数：`**
>
> master_host：主数据库的IP地址；
>
> master_port：主数据库的运行端口；
>
> master_user：在主数据库创建的用于同步数据的用户账号；
>
> master_password：在主数据库创建的用于同步数据的用户密码；
>
> master_log_file：指定从数据库要复制数据的日志文件，通过查看主数据的状态，获取File参数；
>
> master_log_pos：指定从数据库从哪个位置开始复制数据，通过查看主数据的状态，获取Position参数；
>
> master_connect_retry：连接失败重试的时间间隔，单位为秒。 

:::



:::tip 10.从数据库查看主从复制状态，开启主从复制

```shell
show slave status \G;
start slave;
```

![image-20220616161252587](https://img.yishenlaoban.top/image_my/2346254-20220616161252823-1024576173.png) 

![image-20220616163401599](https://img.yishenlaoban.top/image_my/2346254-20220616163402388-1182820959.png) 

:::



:::tip 11.主从复制测试

主机创建数据库--> 创建表--->插入数据

```shell
create database db1;
use db1;
create table t1(id int,name varchar(20));
```

![image-20220616164424953](https://img.yishenlaoban.top/image_my/2346254-20220616164425279-629580441.png) 

检查从数据库的数据是否同步了

![image-20220616165354941](https://img.yishenlaoban.top/image_my/2346254-20220616165355603-52925751.png) 

:::



### 2、Redis分布式存储

:::warning 面试题

**`1~2亿条数据需要缓存，请问如何设计这个存储案例？：分布式存储`**

:::

#### 2.1、理论

**这里有三种解决方案**：

> **哈希取余分区**
>
> ![image-20220617135700404](https://img.yishenlaoban.top/image_my/2346254-20220617135701165-900471148.png) 
>
> 2亿条记录就是2亿个k,v，我们单机不行必须要分布式多机，假设有3台机器构成一个集群，用户每次读写操作都是根据公式：
>
> **hash(key) % N**个机器台数，计算出哈希值，用来决定数据映射到哪一个节点上。
>
> * **优点：**
>
> ​         简单粗暴，直接有效，只需要预估好数据规划好节点，例如3台、8台、10台，就能保证一段时间的数据支撑。使用Hash算法让固定的一部分请求落到同一台服务器上，这样每台服务器固定处理一部分请求（并维护这些请求的信息），起到负载均衡+分而治之的作用。
>
> * **缺点：**
>
> ​         原来规划好的节点，进行扩容或者缩容就比较麻烦了额，不管扩缩，每次数据变动导致节点有变动，映射关系需要重新进行计算，在服务器个数固定不变时没有问题，如果需要弹性扩容或故障停机的情况下，原来的取模公式就会发生变化：**Hash(key)/3会变成Hash(key) /?**。此时地址经过取余运算的结果将发生很大变化，根据公式获取的服务器也会变得不可控。某个redis机器宕机了，由于台数数量变化，会导致hash取余全部数据重新洗牌。

:::tip 一致性哈希算法分区

**`一致性Hash算法背景`**

　　一致性哈希算法在1997年由麻省理工学院中提出的，设计目标是为了解决

分布式缓存数据变动和映射问题，某个机器宕机了，分母数量改变了，自然取余数不OK了。

**`目的`**：当服务个数发生变化时，尽量减少影响客户端到服务端的映射关系。



**`3大步骤：`**

* **构建一致性哈希环**

**一致性哈希环**

  一致性哈希算法必然有个hash函数并按照算法产生hash值，**这个算法的所有可能哈希值会构成一个全量集**，这个集合可以成为一个hash空间**[0,2^32-1]**，这个是一个线性空间，但是在算法中，我们通过适当的逻辑控制将它首尾相连(0 = 2^32),这样让它逻辑上形成了一个环形空间。 

​     它也是按照使用取模的方法，前面笔记介绍的节点取模法是对节点（服务器）的数量进行取模。而一致性Hash算法是对2^32 取模，简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32-1 （即哈希值是一个32位无符号整形），整个哈希环如下图：整个空间按顺时针方向组织，圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、……直到2^32-1， 也就是说0点左侧的第一个点代表2^32-1， 0和2^32-1 在零点中方向重合，我们把这个由2^32个点组成的圆环称为Hash环。

![image-20220617142409616](https://img.yishenlaoban.top/image_my/2346254-20220617142410245-806465554.png) 

* **服务器IP节点映射**

​         将集群中各个IP节点映射到环上的某一个位置。

​        将各个服务器使用Hash进行一个哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置。假如4个节点**NodeA、B、C、D，经过IP地址的哈希函数计(hash(ip))**，使用IP地址哈希后在环空间的位置如下： 

![image-20220617142553512](https://img.yishenlaoban.top/image_my/2346254-20220617142554095-1120884041.png) 

* **key落到服务器的落键规则：**

​         当我们需要存储一个kv键值对时，首先计算key的hash值，hash(key)，将这个key使用相同的函数Hash计算出哈希值并确定此数据在环上的位置，**从此位置沿环顺时针“行走”**，第一台遇到的服务器就是其应该定位到的服务器，并将该键值对存储在该节点上。

​         如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下：根据一致性Hash算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。

![image-20220617142837992](https://img.yishenlaoban.top/image_my/2346254-20220617142838674-661053997.png) 



**`优点`**

* **容错性**

​        假设Node C宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性Hash算法中，如果一台服务器不可用，则**受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据**，其它不会受到影响。简单说，就是C挂了，受到影响的只是B、C之间的数据，并且这些数据会转移到D进行存储。

![image-20220617143402774](https://img.yishenlaoban.top/image_my/2346254-20220617143403338-1986461479.png) 

* **扩展性**

数据量增加了，需要增加一台节点NodeX，X的位置在A和B之间，那收到影响的也就是A到X之间的数据，重新把A到X的数据录入到X上即可，

不会导致hash取余全部数据重新洗牌。

![image-20220617143434284](https://img.yishenlaoban.top/image_my/2346254-20220617143434902-957846474.png) 



**`缺点`**：

**Hash环的数据倾斜问题**

一致性Hash算法在服务**节点太少时**，容易因为节点分布不均匀而造成**数据倾斜**（被缓存的对象大部分集中缓存在某一台服务器上）问题，

例如系统中只有两台服务器：

![image-20220617143530273](https://img.yishenlaoban.top/image_my/2346254-20220617143530995-1634590130.png) 

:::



:::danger 哈希槽分区

* **目的**

解决一致性哈希算法的**数据倾斜问题**

哈希槽实质就是一个数组，数组[0,2^14 -1]形成hash slot空间。

* **原理**

​        解决均匀分配的问题，**在数据和节点之间又加入了一层，把这层称为哈希槽（slot），用于管理数据和节点之间的关系**，现在就相当于节点上放的是槽，槽里放的是数据。

![img](https://img.yishenlaoban.top/image_my/2346254-20220618135928431-346234236.png) 

槽解决的是粒度问题，相当于把粒度变大了，这样便于数据移动。

哈希解决的是映射问题，使用key的哈希值来计算所在的槽，便于数据分配。

* **槽的个数**

​         一个集群只能有16384个槽，编号**0-16383（0-2^14-1）**。这些槽会分配给集群中的所有主节点，分配策略没有要求。可以指定哪些编号的槽分配给哪个主节点。集群会记录节点和槽的对应关系。解决了节点和槽的关系后，接下来就需要对key求哈希值，然后对16384取余，余数是几key就落入对应的槽里。**slot = CRC16(key) % 16384**。以槽为单位移动数据，因为槽的数目是固定的，处理起来比较容易，这样数据移动问题就解决了。

* **操作**

​       Redis 集群中内置了 16384 个哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。当需要在 Redis 集群中放置一个 key-value时，**redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数**，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，也就是映射到某个节点上。如下代码，key之A 、B在Node2， key之C落在Node3上

![image-20220617151018609](https://img.yishenlaoban.top/image_my/2346254-20220617151019316-33035327.png) 

![image-20220617151031763](https://img.yishenlaoban.top/image_my/2346254-20220617151032592-1447169908.png)  

:::



#### 2.2、部署3主3从redis

![image-20220617155517604](https://img.yishenlaoban.top/image_my/2346254-20220617155518165-2038351089.png) 



*  **启动运行容器**

```shell
docker run -d  #创建并后台运行docker容器实例
--name redis-node-1 #容器名
--net host #使用宿主机的IP和端口，默认
--privileged=true #获取宿主机root用户权限
-v /data/redis/share/redis-node-1:/data #容器数据卷，宿主机地址：docker内部地址
redis:6.0.8 #redis镜像和版本
--cluster-enabled yes #开启redis集群
--appendonly yes #开启持久化
--port 6381 #redis端口号

docker run -d --name redis-node-2 --net host --privileged=true -v /data/redis/share/redis-node-2:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6382

docker run -d --name redis-node-3 --net host --privileged=true -v /data/redis/share/redis-node-3:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6383

docker run -d --name redis-node-4 --net host --privileged=true -v /data/redis/share/redis-node-4:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6384

docker run -d --name redis-node-5 --net host --privileged=true -v /data/redis/share/redis-node-5:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6385

docker run -d --name redis-node-6 --net host --privileged=true -v /data/redis/share/redis-node-6:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6386

```

![image-20220617160723372](https://img.yishenlaoban.top/image_my/2346254-20220617160724120-1665812226.png) 



* **构建主从关系**

进入node1

```shell
docker exec -it redis-node-1 /bin/bash
```

构建关系(ip是宿主机ip)

```shell
#注意，进入docker容器后才能执行一下命令，且注意自己的真实IP地址
redis-cli --cluster create 211.69.238.84:6381 211.69.238.84:6382 211.69.238.84:6383 211.69.238.84:6384 211.69.238.84:6385 211.69.238.84:6386 --cluster-replicas 1
```

```shell
zcs@ccc:~$ docker exec -it redis-node-1 /bin/bash
root@ccc:/data# redis-cli --cluster create 211.69.238.84:6381 211.69.238.84:6382 211.69.238.84:6383 211.69.238.84:6384 211.69.238.84:6385 211.69.238.84:6386 --cluster-replicas 1
>>> Performing hash slots allocation on 6 nodes...
Master[0] -> Slots 0 - 5460
Master[1] -> Slots 5461 - 10922
Master[2] -> Slots 10923 - 16383
Adding replica 211.69.238.84:6385 to 211.69.238.84:6381
Adding replica 211.69.238.84:6386 to 211.69.238.84:6382
Adding replica 211.69.238.84:6384 to 211.69.238.84:6383
>>> Trying to optimize slaves allocation for anti-affinity
[WARNING] Some slaves are in the same host as their master
M: 3992f6db5936678c6c9d14093f0fb00a2e1c0e11 211.69.238.84:6381
   slots:[0-5460] (5461 slots) master
M: cc249a52ae69a99b703507d7377c614060e9be7e 211.69.238.84:6382
   slots:[5461-10922] (5462 slots) master
M: ddfd56912411e613918e87b5e1390ec03fd4e785 211.69.238.84:6383
   slots:[10923-16383] (5461 slots) master
S: 9c4d2f55526326c11317cbb9202fb63fb023c31c 211.69.238.84:6384
   replicates 3992f6db5936678c6c9d14093f0fb00a2e1c0e11
S: 58f9b5a44621ad2d0d22dc0ccfb630a11684aa8c 211.69.238.84:6385
   replicates cc249a52ae69a99b703507d7377c614060e9be7e
S: 942b15e1022cbcf3b08d209555e5602a1f1b8421 211.69.238.84:6386
   replicates ddfd56912411e613918e87b5e1390ec03fd4e785
Can I set the above configuration? (type 'yes' to accept): yes
>>> Nodes configuration updated
>>> Assign a different config epoch to each node
>>> Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join
.
>>> Performing Cluster Check (using node 211.69.238.84:6381)
M: 3992f6db5936678c6c9d14093f0fb00a2e1c0e11 211.69.238.84:6381
   slots:[0-5460] (5461 slots) master
   1 additional replica(s)
S: 942b15e1022cbcf3b08d209555e5602a1f1b8421 211.69.238.84:6386
   slots: (0 slots) slave
   replicates ddfd56912411e613918e87b5e1390ec03fd4e785
S: 9c4d2f55526326c11317cbb9202fb63fb023c31c 211.69.238.84:6384
   slots: (0 slots) slave
   replicates 3992f6db5936678c6c9d14093f0fb00a2e1c0e11
M: cc249a52ae69a99b703507d7377c614060e9be7e 211.69.238.84:6382
   slots:[5461-10922] (5462 slots) master
   1 additional replica(s)
S: 58f9b5a44621ad2d0d22dc0ccfb630a11684aa8c 211.69.238.84:6385
   slots: (0 slots) slave
   replicates cc249a52ae69a99b703507d7377c614060e9be7e
M: ddfd56912411e613918e87b5e1390ec03fd4e785 211.69.238.84:6383
   slots:[10923-16383] (5461 slots) master
   1 additional replica(s)
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.

```

 

* **查看信息**

通过6381这个结点查看集群状态：

```shell
redis-cli -p 6381
cluster info
```

![image-20220617162527737](https://img.yishenlaoban.top/image_my/2346254-20220617162528322-1341943946.png)  

:::danger

可以看到总结点个数以及从机的数量

:::



通过下面的命令，查看具体的主、从机分别是哪个服务

```shell
cluster nodes
```

```shell
127.0.0.1:6381> cluster nodes
942b15e1022cbcf3b08d209555e5602a1f1b8421 211.69.238.84:6386@16386 slave ddfd56912411e613918e87b5e1390ec03fd4e785 0 1655454377872 3 connected
9c4d2f55526326c11317cbb9202fb63fb023c31c 211.69.238.84:6384@16384 slave 3992f6db5936678c6c9d14093f0fb00a2e1c0e11 0 1655454375858 1 connected
3992f6db5936678c6c9d14093f0fb00a2e1c0e11 211.69.238.84:6381@16381 myself,master - 0 1655454377000 1 connected 0-5460
cc249a52ae69a99b703507d7377c614060e9be7e 211.69.238.84:6382@16382 master - 0 1655454378879 2 connected 5461-10922
58f9b5a44621ad2d0d22dc0ccfb630a11684aa8c 211.69.238.84:6385@16385 slave cc249a52ae69a99b703507d7377c614060e9be7e 0 1655454378000 2 connected
ddfd56912411e613918e87b5e1390ec03fd4e785 211.69.238.84:6383@16383 master - 0 1655454377000 3 connected 10923-16383

```

:::danger

slave 后面跟的id就是这个机器的master的id

它们的关系是：

* 6381(m)->6384(s)
* 6382(m)->6385(c)
* 6383(m)->6386(c)

这个主从关系是随机分配的，并不是指定顺序的

:::



* 测试

在结点1使用set命令进行存储测试，发现以下问题

```shell
127.0.0.1:6381> set k1 v1
(error) MOVED 12706 211.69.238.84:6383
127.0.0.1:6381> set ke v2
(error) MOVED 11219 211.69.238.84:6383
127.0.0.1:6381> set k3 v3
OK
127.0.0.1:6381> set k4 v4
(error) MOVED 8455 211.69.238.84:6382
```

![image-20220617164830343](https://img.yishenlaoban.top/image_my/2346254-20220617164830968-684219661.png) 

:::danger

因为我们仍然使用的是

```shell
redis-cli -p 6381
```

命令进入的第一个结点，而存的时候，结点1能接受的哈希槽在0-5460之间，上面两个8455、12706都是超过了5460，所以储存失败

:::

:::tip 

所以这里我们要使用集群连接

```shell
redis-cli -p 6381 -c
```

```shell
root@ccc:/data# redis-cli -p 6381 -c
127.0.0.1:6381> set k1 v1 
-> Redirected to slot [12706] located at 211.69.238.84:6383 #转到了节点3
OK
211.69.238.84:6383> set k2 v2 
-> Redirected to slot [449] located at 211.69.238.84:6381 #转到了节点1
OK
211.69.238.84:6381> set k3 v3 
OK

```

:::



* 查看集群信息

```shell
redis-cli --cluster check 211.69.238.84:6381
```

```shell
root@ccc:/data# redis-cli --cluster check 211.69.238.84:6381
211.69.238.84:6381 (3992f6db...) -> 2 keys | 5461 slots | 1 slaves.
211.69.238.84:6382 (cc249a52...) -> 0 keys | 5462 slots | 1 slaves.
211.69.238.84:6383 (ddfd5691...) -> 1 keys | 5461 slots | 1 slaves.
[OK] 3 keys in 3 masters.
0.00 keys per slot on average.
>>> Performing Cluster Check (using node 211.69.238.84:6381)
M: 3992f6db5936678c6c9d14093f0fb00a2e1c0e11 211.69.238.84:6381
   slots:[0-5460] (5461 slots) master
   1 additional replica(s)
S: 942b15e1022cbcf3b08d209555e5602a1f1b8421 211.69.238.84:6386
   slots: (0 slots) slave
   replicates ddfd56912411e613918e87b5e1390ec03fd4e785
S: 9c4d2f55526326c11317cbb9202fb63fb023c31c 211.69.238.84:6384
   slots: (0 slots) slave
   replicates 3992f6db5936678c6c9d14093f0fb00a2e1c0e11
M: cc249a52ae69a99b703507d7377c614060e9be7e 211.69.238.84:6382
   slots:[5461-10922] (5462 slots) master
   1 additional replica(s)
S: 58f9b5a44621ad2d0d22dc0ccfb630a11684aa8c 211.69.238.84:6385
   slots: (0 slots) slave
   replicates cc249a52ae69a99b703507d7377c614060e9be7e
M: ddfd56912411e613918e87b5e1390ec03fd4e785 211.69.238.84:6383
   slots:[10923-16383] (5461 slots) master
   1 additional replica(s)
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
```



#### 2.3、主从迁移案例

![image-20220617171806120](https://img.yishenlaoban.top/image_my/2346254-20220617171807544-350333521.png) 

**测试node1宕机，cluster切换**

* 停掉node1

```shell
docker stop redis-node-1
```

```shell
zcs@ccc:~$ docker ps
CONTAINER ID   IMAGE         COMMAND                  CREATED             STATUS             PORTS     NAMES
88aa564d00db   redis:6.0.8   "docker-entrypoint.s…"   About an hour ago   Up About an hour             redis-node-6
0fd8e798327e   redis:6.0.8   "docker-entrypoint.s…"   About an hour ago   Up About an hour             redis-node-5
cea260e0b87d   redis:6.0.8   "docker-entrypoint.s…"   About an hour ago   Up About an hour             redis-node-4
8fccd1633fd0   redis:6.0.8   "docker-entrypoint.s…"   About an hour ago   Up About an hour             redis-node-3
560987b19df5   redis:6.0.8   "docker-entrypoint.s…"   About an hour ago   Up About an hour             redis-node-2
4da7dc050a24   redis:6.0.8   "docker-entrypoint.s…"   About an hour ago   Up About an hour             redis-node-1
zcs@ccc:~$ docker stop redis-node-1
redis-node-1
zcs@ccc:~$ docker ps
CONTAINER ID   IMAGE         COMMAND                  CREATED             STATUS             PORTS     NAMES
88aa564d00db   redis:6.0.8   "docker-entrypoint.s…"   About an hour ago   Up About an hour             redis-node-6
0fd8e798327e   redis:6.0.8   "docker-entrypoint.s…"   About an hour ago   Up About an hour             redis-node-5
cea260e0b87d   redis:6.0.8   "docker-entrypoint.s…"   About an hour ago   Up About an hour             redis-node-4
8fccd1633fd0   redis:6.0.8   "docker-entrypoint.s…"   About an hour ago   Up About an hour             redis-node-3
560987b19df5   redis:6.0.8   "docker-entrypoint.s…"   About an hour ago   Up About an hour             redis-node-2
```



* 查看集群状态

```shell
zcs@ccc:~$ docker exec -it redis-node-2 bash
root@ccc:/data# redis-cli -p 6382 -c
127.0.0.1:6382> cluster nodes
```

![image-20220617172613284](https://img.yishenlaoban.top/image_my/2346254-20220617172613866-918499658.png) 

:::danger

可以发现，与node1对应的cluster node4，端口6386的结点，已经替代node1成为了master

:::



* 恢复node1发生的变化

重新启动

```shell
docker start redis-node-1
```

![image-20220617172837186](https://img.yishenlaoban.top/image_my/2346254-20220617172837807-1037496916.png) 

在node2查看集群状态

```shell
zcs@ccc:~$ docker exec -it redis-node-2 bash
root@ccc:/data# redis-cli -p 6382 -c
127.0.0.1:6382> cluster nodes
58f9b5a44621ad2d0d22dc0ccfb630a11684aa8c 211.69.238.84:6385@16385 slave cc249a52ae69a99b703507d7377c614060e9be7e 0 1655458167391 2 connected
cc249a52ae69a99b703507d7377c614060e9be7e 211.69.238.84:6382@16382 myself,master - 0 1655458166000 2 connected 5461-10922
942b15e1022cbcf3b08d209555e5602a1f1b8421 211.69.238.84:6386@16386 slave ddfd56912411e613918e87b5e1390ec03fd4e785 0 1655458166000 3 connected
9c4d2f55526326c11317cbb9202fb63fb023c31c 211.69.238.84:6384@16384 master - 0 1655458166384 7 connected 0-5460
3992f6db5936678c6c9d14093f0fb00a2e1c0e11 211.69.238.84:6381@16381 slave 9c4d2f55526326c11317cbb9202fb63fb023c31c 0 1655458164000 7 connected
ddfd56912411e613918e87b5e1390ec03fd4e785 211.69.238.84:6383@16383 master - 0 1655458166000 3 connected 10923-16383
```

:::danger

可以发现，重新启动node1后，它并没有重新成为之前的master，而是成为了node4的slave

:::

:::warning 总结

redis主从容错切换迁移有一套自己的规则，当一个master 宕机后，它的slave就会成为新的master,当它重新启动，它并没有重新回到master位置，而是成为了新的master的slave。

:::